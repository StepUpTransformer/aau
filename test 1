#include <iostream>
#include <cuda_runtime.h>
#include <curand_kernel.h>

#define INPUT_SIZE 100
#define HIDDEN_SIZE 200
#define OUTPUT_SIZE 100

// CUDA kernel for the MLP forward pass
__global__ void forward_pass(float *input, float *hidden, float *output,
                              float *W1, float *b1, float *W2, float *b2,
                              int input_size, int hidden_size, int output_size) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;

    // Hidden layer computation: input * W1 + b1
    if (idx < hidden_size) {
        float sum = 0.0f;
        for (int i = 0; i < input_size; ++i) {
            sum += input[i] * W1[i * hidden_size + idx];
        }
        hidden[idx] = tanh(sum + b1[idx]); // Apply activation function (tanh)
    }

    __syncthreads();

    // Output layer computation: hidden * W2 + b2
    if (idx < output_size) {
        float sum = 0.0f;
        for (int i = 0; i < hidden_size; ++i) {
            sum += hidden[i] * W2[i * output_size + idx];
        }
        output[idx] = sum + b2[idx];
    }
}

// CUDA kernel for softmax activation function
__global__ void softmax(float *output, float *softmax_out, int output_size) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < output_size) {
        float max_val = output[0];
        for (int i = 1; i < output_size; ++i) {
            max_val = max(max_val, output[i]);
        }

        float sum_exp = 0.0f;
        for (int i = 0; i < output_size; ++i) {
            sum_exp += expf(output[i] - max_val);
        }

        softmax_out[idx] = expf(output[idx] - max_val) / sum_exp;
    }
}

int main() {
    float *input, *hidden, *output;
    float *W1, *b1, *W2, *b2;
    float *d_input, *d_hidden, *d_output, *d_W1, *d_b1, *d_W2, *d_b2, *d_softmax_out;

    // Allocate memory for input, hidden layer, and output
    input = new float[INPUT_SIZE];
    hidden = new float[HIDDEN_SIZE];
    output = new float[OUTPUT_SIZE];

    // Allocate memory for weights and biases
    W1 = new float[INPUT_SIZE * HIDDEN_SIZE];
    b1 = new float[HIDDEN_SIZE];
    W2 = new float[HIDDEN_SIZE * OUTPUT_SIZE];
    b2 = new float[OUTPUT_SIZE];

    // Initialize input, weights, and biases with random values
    for (int i = 0; i < INPUT_SIZE; ++i) {
        input[i] = rand() / (float)RAND_MAX;
    }

    for (int i = 0; i < HIDDEN_SIZE; ++i) {
        b1[i] = rand() / (float)RAND_MAX;
        for (int j = 0; j < INPUT_SIZE; ++j) {
            W1[i * INPUT_SIZE + j] = rand() / (float)RAND_MAX;
        }
    }

    for (int i = 0; i < OUTPUT_SIZE; ++i) {
        b2[i] = rand() / (float)RAND_MAX;
        for (int j = 0; j < HIDDEN_SIZE; ++j) {
            W2[i * HIDDEN_SIZE + j] = rand() / (float)RAND_MAX;
        }
    }

    // Allocate memory on the device
    cudaMalloc((void**)&d_input, INPUT_SIZE * sizeof(float));
    cudaMalloc((void**)&d_hidden, HIDDEN_SIZE * sizeof(float));
    cudaMalloc((void**)&d_output, OUTPUT_SIZE * sizeof(float));
    cudaMalloc((void**)&d_W1, INPUT_SIZE * HIDDEN_SIZE * sizeof(float));
    cudaMalloc((void**)&d_b1, HIDDEN_SIZE * sizeof(float));
    cudaMalloc((void**)&d_W2, HIDDEN_SIZE * OUTPUT_SIZE * sizeof(float));
    cudaMalloc((void**)&d_b2, OUTPUT_SIZE * sizeof(float));
    cudaMalloc((void**)&d_softmax_out, OUTPUT_SIZE * sizeof(float));

    // Copy data to the device
    cudaMemcpy(d_input, input, INPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W1, W1, INPUT_SIZE * HIDDEN_SIZE * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b1, b1, HIDDEN_SIZE * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W2, W2, HIDDEN_SIZE * OUTPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b2, b2, OUTPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);

    // Launch kernel for forward pass
    forward_pass<<<(HIDDEN_SIZE + 255) / 256, 256>>>(d_input, d_hidden, d_output,
                                                     d_W1, d_b1, d_W2, d_b2,
                                                     INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE);

    // Launch kernel for softmax
    softmax<<<(OUTPUT_SIZE + 255) / 256, 256>>>(d_output, d_softmax_out, OUTPUT_SIZE);

    // Copy the result back to the host
    cudaMemcpy(output, d_softmax_out, OUTPUT_SIZE * sizeof(float), cudaMemcpyDeviceToHost);

    // Print the softmax output
    std::cout << "Softmax Output:" << std::endl;
    for (int i = 0; i < OUTPUT_SIZE; ++i) {
        std::cout << output[i] << " ";
    }
    std::cout << std::endl;

    // Free memory
    delete[] input;
    delete[] hidden;
    delete[] output;
    delete[] W1;
    delete[] b1;
    delete[] W2;
    delete[] b2;

    cudaFree(d_input);
    cudaFree(d_hidden);
    cudaFree(d_output);
    cudaFree(d_W1);
    cudaFree(d_b1);
    cudaFree(d_W2);
    cudaFree(d_b2);
    cudaFree(d_softmax_out);

    return 0;
}



#include <iostream>
#include <cuda_runtime.h>
#include <curand_kernel.h>
#include <cuda_fp16.h>  // For 16-bit floating point support

#define INPUT_SIZE 100
#define HIDDEN_SIZE 200
#define OUTPUT_SIZE 100

// CUDA kernel for the MLP forward pass with half-precision floating point (FP16)
__global__ void forward_pass(half *input, half *hidden, half *output,
                              half *W1, half *b1, half *W2, half *b2,
                              int input_size, int hidden_size, int output_size) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;

    // Hidden layer computation: input * W1 + b1
    if (idx < hidden_size) {
        half sum = __float2half(0.0f);
        for (int i = 0; i < input_size; ++i) {
            sum = __hadd(sum, __hmul(input[i], W1[i * hidden_size + idx]));
        }
        hidden[idx] = __h_tanh(__hadd(sum, b1[idx]));  // Apply activation function (tanh)
    }

    __syncthreads();

    // Output layer computation: hidden * W2 + b2
    if (idx < output_size) {
        half sum = __float2half(0.0f);
        for (int i = 0; i < hidden_size; ++i) {
            sum = __hadd(sum, __hmul(hidden[i], W2[i * output_size + idx]));
        }
        output[idx] = __hadd(sum, b2[idx]);
    }
}

// CUDA kernel for softmax activation function with FP16
__global__ void softmax(half *output, half *softmax_out, int output_size) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < output_size) {
        half max_val = output[0];
        for (int i = 1; i < output_size; ++i) {
            max_val = __hgt(output[i], max_val) ? output[i] : max_val;
        }

        half sum_exp = __float2half(0.0f);
        for (int i = 0; i < output_size; ++i) {
            sum_exp = __hadd(sum_exp, __hexp(output[i] - max_val));
        }

        softmax_out[idx] = __hdiv(__hexp(output[idx] - max_val), sum_exp);
    }
}

int main() {
    half *input, *hidden, *output;
    half *W1, *b1, *W2, *b2;
    half *d_input, *d_hidden, *d_output, *d_W1, *d_b1, *d_W2, *d_b2, *d_softmax_out;

    // Allocate memory for input, hidden layer, and output
    input = new half[INPUT_SIZE];
    hidden = new half[HIDDEN_SIZE];
    output = new half[OUTPUT_SIZE];

    // Allocate memory for weights and biases
    W1 = new half[INPUT_SIZE * HIDDEN_SIZE];
    b1 = new half[HIDDEN_SIZE];
    W2 = new half[HIDDEN_SIZE * OUTPUT_SIZE];
    b2 = new half[OUTPUT_SIZE];

    // Initialize input, weights, and biases with random values
    for (int i = 0; i < INPUT_SIZE; ++i) {
        input[i] = __float2half(rand() / (float)RAND_MAX);
    }

    for (int i = 0; i < HIDDEN_SIZE; ++i) {
        b1[i] = __float2half(rand() / (float)RAND_MAX);
        for (int j = 0; j < INPUT_SIZE; ++j) {
            W1[i * INPUT_SIZE + j] = __float2half(rand() / (float)RAND_MAX);
        }
    }

    for (int i = 0; i < OUTPUT_SIZE; ++i) {
        b2[i] = __float2half(rand() / (float)RAND_MAX);
        for (int j = 0; j < HIDDEN_SIZE; ++j) {
            W2[i * HIDDEN_SIZE + j] = __float2half(rand() / (float)RAND_MAX);
        }
    }

    // Allocate memory on the device
    cudaMalloc((void**)&d_input, INPUT_SIZE * sizeof(half));
    cudaMalloc((void**)&d_hidden, HIDDEN_SIZE * sizeof(half));
    cudaMalloc((void**)&d_output, OUTPUT_SIZE * sizeof(half));
    cudaMalloc((void**)&d_W1, INPUT_SIZE * HIDDEN_SIZE * sizeof(half));
    cudaMalloc((void**)&d_b1, HIDDEN_SIZE * sizeof(half));
    cudaMalloc((void**)&d_W2, HIDDEN_SIZE * OUTPUT_SIZE * sizeof(half));
    cudaMalloc((void**)&d_b2, OUTPUT_SIZE * sizeof(half));
    cudaMalloc((void**)&d_softmax_out, OUTPUT_SIZE * sizeof(half));

    // Copy data to the device
    cudaMemcpy(d_input, input, INPUT_SIZE * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W1, W1, INPUT_SIZE * HIDDEN_SIZE * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b1, b1, HIDDEN_SIZE * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_W2, W2, HIDDEN_SIZE * OUTPUT_SIZE * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b2, b2, OUTPUT_SIZE * sizeof(half), cudaMemcpyHostToDevice);

    // Launch kernel for forward pass
    forward_pass<<<(HIDDEN_SIZE + 255) / 256, 256>>>(d_input, d_hidden, d_output,
                                                     d_W1, d_b1, d_W2, d_b2,
                                                     INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE);

    // Launch kernel for softmax
    softmax<<<(OUTPUT_SIZE + 255) / 256, 256>>>(d_output, d_softmax_out, OUTPUT_SIZE);

    // Copy the result back to the host
    cudaMemcpy(output, d_softmax_out, OUTPUT_SIZE * sizeof(half), cudaMemcpyDeviceToHost);

    // Print the softmax output
    std::cout << "Softmax Output:" << std::endl;
    for (int i = 0; i < OUTPUT_SIZE; ++i) {
        std::cout << __half2float(output[i]) << " ";
    }
    std::cout << std::endl;

    // Free memory
    delete[] input;
    delete[] hidden;
    delete[] output;
    delete[] W1;
    delete[] b1;
    delete[] W2;
    delete[] b2;

    cudaFree(d_input);
    cudaFree(d_hidden);
    cudaFree(d_output);
    cudaFree(d_W1);
    cudaFree(d_b1);
    cudaFree(d_W2);
    cudaFree(d_b2);
    cudaFree(d_softmax_out);

    return 0;
}



__global__ void softmax(half *output, half *softmax_out, int output_size) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    if (idx < output_size) {
        // Find the maximum value to stabilize the softmax calculation
        half max_val = output[0];
        for (int i = 1; i < output_size; ++i) {
            max_val = __hgt(output[i], max_val) ? output[i] : max_val;
        }

        // Compute the sum of exponentials
        half sum_exp = __float2half(0.0f);
        for (int i = 0; i < output_size; ++i) {
            // Convert from half to float, compute exp, then convert back to half
            float exp_val = expf(__half2float(output[i]) - __half2float(max_val));
            sum_exp = __hadd(sum_exp, __float2half(exp_val));
        }

        // Compute softmax output
        softmax_out[idx] = __hdiv(__float2half(__half2float(output[idx]) - __half2float(max_val)), sum_exp);
    }
}


import torch
import torch.nn as nn
import torch.nn.functional as F

# Define the MLP class
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer
        self.fc2 = nn.Linear(hidden_size, output_size)  # Hidden to output layer

    def forward(self, x):
        x = torch.tanh(self.fc1(x))  # Hidden layer with tanh activation
        x = self.fc2(x)  # Output layer (raw scores)
        return F.softmax(x, dim=1)  # Apply softmax to get probabilities

# Set random seed for reproducibility
torch.manual_seed(42)

# Initialize the model with the desired sizes
input_size = 100
hidden_size = 200
output_size = 100

# Create a random input tensor
input_tensor = torch.rand(input_size)

# Instantiate the MLP model
model = MLP(input_size, hidden_size, output_size)

# Perform a forward pass
output = model(input_tensor)

# Print the softmax output (probabilities)
print("Softmax Output:")
print(output)


import torch

class TinyModel(torch.nn.Module):
    def __init__(self):
        super(TinyModel, self).__init__()
        
        # Define layers
        self.linear1 = torch.nn.Linear(10000, 20000, dtype=torch.float16, device='cuda:0')
        self.activation = torch.nn.ReLU()
        self.linear2 = torch.nn.Linear(20000, 10, dtype=torch.float16, device='cuda:0')
        self.softmax = torch.nn.Softmax(dim=0)
        
        # Initialize weights and biases
        self._initialize_weights()
    
    def _initialize_weights(self):
        # Custom weight initialization
        with torch.no_grad():
            for layer in [self.linear1, self.linear2]:
                # For each layer, initialize the weights and biases
                for i in range(layer.weight.size(0)):  # Iterate over rows
                    for j in range(layer.weight.size(1)):  # Iterate over columns
                        # Set weights using the formula (row * column * 0.001) + 0.01
                        layer.weight[i, j] = (i * j * 0.001) + 0.01
                # Set biases to 0.005
                layer.bias.fill_(0.005)

# Example usage
model = TinyModel()



#include <iostream>
#include <cstdlib>
#include <ctime>

#define N 1024  // Matrix dimensions, modify as needed
#define TILE_SIZE 32  // Tile size for optimization

// Kernel to multiply matrices A and B, and store the result in C
__global__ void matMulKernel(float *A, float *B, float *C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;

    if (row < N && col < N) {
        for (int i = 0; i < N; i++) {
            sum += A[row * N + i] * B[i * N + col];
        }
        C[row * N + col] = sum;
    }
}

void randomMatrix(float *matrix, int size) {
    for (int i = 0; i < size; i++) {
        matrix[i] = static_cast<float>(rand()) / RAND_MAX; // Random values between 0 and 1
    }
}

int main() {
    srand(time(0));

    float *A, *B, *C;  // Matrices
    float *d_A, *d_B, *d_C;  // Device matrices

    // Allocate memory for host matrices
    A = (float*)malloc(N * N * sizeof(float));
    B = (float*)malloc(N * N * sizeof(float));
    C = (float*)malloc(N * N * sizeof(float));

    // Generate random matrices A and B
    randomMatrix(A, N * N);
    randomMatrix(B, N * N);

    // Allocate memory for device matrices
    cudaMalloc((void**)&d_A, N * N * sizeof(float));
    cudaMalloc((void**)&d_B, N * N * sizeof(float));
    cudaMalloc((void**)&d_C, N * N * sizeof(float));

    // Copy matrices A and B to device memory
    cudaMemcpy(d_A, A, N * N * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, B, N * N * sizeof(float), cudaMemcpyHostToDevice);

    // Launch kernel to multiply matrices
    dim3 blockDim(TILE_SIZE, TILE_SIZE);
    dim3 gridDim((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);
    matMulKernel<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);

    // Copy result matrix C back to host
    cudaMemcpy(C, d_C, N * N * sizeof(float), cudaMemcpyDeviceToHost);

    // Check for errors
    cudaDeviceSynchronize();
    cudaError_t error = cudaGetLastError();
    if (error != cudaSuccess) {
        std::cerr << "CUDA error: " << cudaGetErrorString(error) << std::endl;
    }

    // Free memory
    free(A);
    free(B);
    free(C);
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    return 0;
}