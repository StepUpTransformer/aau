#include <cuda_runtime.h>
#include <iostream>
#include <vector>
#include <cmath>
#include <cstdlib>

#define BLOCK_SIZE 256

// CUDA Kernel for matrix-vector multiplication with bias
__global__ void matVecMulBias(const float* mat, const float* vec, const float* bias, float* output, int rows, int cols) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < rows) {
        float sum = bias[row];
        for (int col = 0; col < cols; ++col) {
            sum += mat[row * cols + col] * vec[col];
        }
        output[row] = sum;
    }
}

// CUDA Kernel for Tanh activation
__global__ void applyTanh(float* data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        data[idx] = tanhf(data[idx]);
    }
}

// CUDA Kernel for Softmax
__global__ void softmax(float* data, int size) {
    __shared__ float sum;
    __shared__ float max_val;

    int idx = threadIdx.x;
    float thread_max = -FLT_MAX;
    float thread_sum = 0.0f;

    // Find max value
    for (int i = idx; i < size; i += blockDim.x) {
        thread_max = fmaxf(thread_max, data[i]);
    }

    if (idx == 0) max_val = thread_max;
    __syncthreads();

    // Exponential and sum
    for (int i = idx; i < size; i += blockDim.x) {
        data[i] = expf(data[i] - max_val);
        thread_sum += data[i];
    }

    if (idx == 0) sum = thread_sum;
    __syncthreads();

    // Normalize
    for (int i = idx; i < size; i += blockDim.x) {
        data[i] /= sum;
    }
}

// Initialize weights and biases
void initializeWeights(float* data, int size) {
    for (int i = 0; i < size; ++i) {
        data[i] = static_cast<float>(rand()) / RAND_MAX;
    }
}

int main() {
    // Model dimensions
    const int input_size = 100;
    const int hidden_size = 200;
    const int output_size = 10;

    // Allocate host memory
    std::vector<float> h_input(input_size);
    std::vector<float> h_hidden(hidden_size);
    std::vector<float> h_output(output_size);

    std::vector<float> h_linear1_weights(input_size * hidden_size);
    std::vector<float> h_linear1_bias(hidden_size);
    std::vector<float> h_linear2_weights(hidden_size * output_size);
    std::vector<float> h_linear2_bias(output_size);

    // Initialize weights and biases
    initializeWeights(h_linear1_weights.data(), input_size * hidden_size);
    initializeWeights(h_linear1_bias.data(), hidden_size);
    initializeWeights(h_linear2_weights.data(), hidden_size * output_size);
    initializeWeights(h_linear2_bias.data(), output_size);
    initializeWeights(h_input.data(), input_size);

    // Allocate device memory
    float *d_input, *d_hidden, *d_output;
    float *d_linear1_weights, *d_linear1_bias;
    float *d_linear2_weights, *d_linear2_bias;

    cudaMalloc(&d_input, input_size * sizeof(float));
    cudaMalloc(&d_hidden, hidden_size * sizeof(float));
    cudaMalloc(&d_output, output_size * sizeof(float));

    cudaMalloc(&d_linear1_weights, input_size * hidden_size * sizeof(float));
    cudaMalloc(&d_linear1_bias, hidden_size * sizeof(float));
    cudaMalloc(&d_linear2_weights, hidden_size * output_size * sizeof(float));
    cudaMalloc(&d_linear2_bias, output_size * sizeof(float));

    // Copy data to device
    cudaMemcpy(d_input, h_input.data(), input_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear1_weights, h_linear1_weights.data(), input_size * hidden_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear1_bias, h_linear1_bias.data(), hidden_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear2_weights, h_linear2_weights.data(), hidden_size * output_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear2_bias, h_linear2_bias.data(), output_size * sizeof(float), cudaMemcpyHostToDevice);

    // Launch kernels
    int numBlocksHidden = (hidden_size + BLOCK_SIZE - 1) / BLOCK_SIZE;
    int numBlocksOutput = (output_size + BLOCK_SIZE - 1) / BLOCK_SIZE;

    matVecMulBias<<<numBlocksHidden, BLOCK_SIZE>>>(d_linear1_weights, d_input, d_linear1_bias, d_hidden, hidden_size, input_size);
    applyTanh<<<numBlocksHidden, BLOCK_SIZE>>>(d_hidden, hidden_size);

    matVecMulBias<<<numBlocksOutput, BLOCK_SIZE>>>(d_linear2_weights, d_hidden, d_linear2_bias, d_output, output_size, hidden_size);
    softmax<<<1, BLOCK_SIZE>>>(d_output, output_size);

    // Copy results back to host
    cudaMemcpy(h_output.data(), d_output, output_size * sizeof(float), cudaMemcpyDeviceToHost);

    // Print output
    for (const auto& val : h_output) {
        std::cout << val << " ";
    }
    std::cout << std::endl;

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_hidden);
    cudaFree(d_output);
    cudaFree(d_linear1_weights);
    cudaFree(d_linear1_bias);
    cudaFree(d_linear2_weights);
    cudaFree(d_linear2_bias);

    return 0;
}