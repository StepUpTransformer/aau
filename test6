#include <cmath>

#define BLOCK_SIZE 256

// CUDA Kernel for matrix-vector multiplication, bias addition, and Tanh activation
__global__ void cuda_mm_tanh(const float* mat, const float* vec, const float* bias, float* output, int rows, int cols) {
    // Shared memory for the vector
    __shared__ float shared_vec[BLOCK_SIZE];

    // Compute row index
    int row = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < rows) {
        float sum = 0.0f;

        // Matrix-vector multiplication
        for (int tile = 0; tile < (cols + BLOCK_SIZE - 1) / BLOCK_SIZE; ++tile) {
            // Load a tile of the vector into shared memory
            int col = tile * BLOCK_SIZE + threadIdx.x;
            if (col < cols) {
                shared_vec[threadIdx.x] = vec[col];
            } else {
                shared_vec[threadIdx.x] = 0.0f;
            }
            __syncthreads();

            // Multiply the row of the matrix with the shared vector tile
            for (int i = 0; i < BLOCK_SIZE; ++i) {
                int col_idx = tile * BLOCK_SIZE + i;
                if (col_idx < cols) {
                    sum += mat[row * cols + col_idx] * shared_vec[i];
                }
            }
            __syncthreads();
        }

        // Add bias and apply Tanh activation
        if (bias) {
            sum += bias[row];
        }
        output[row] = tanhf(sum);
    }
}

// Host code to invoke the optimized kernel
void run_optimized_cuda_kernels(const float* mat, const float* vec, const float* bias, float* output, int rows, int cols) {
    // Define grid and block dimensions
    dim3 blockDim(BLOCK_SIZE);
    dim3 gridDim((rows + BLOCK_SIZE - 1) / BLOCK_SIZE);

    // Allocate memory on the GPU
    float *d_mat, *d_vec, *d_bias, *d_output;
    cudaMalloc(&d_mat, rows * cols * sizeof(float));
    cudaMalloc(&d_vec, cols * sizeof(float));
    cudaMalloc(&d_bias, rows * sizeof(float));
    cudaMalloc(&d_output, rows * sizeof(float));

    // Copy data to GPU
    cudaMemcpy(d_mat, mat, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vec, vec, cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, rows * sizeof(float), cudaMemcpyHostToDevice);

    // Launch optimized kernel
    cuda_mm_tanh<<<gridDim, blockDim>>>(d_mat, d_vec, d_bias, d_output, rows, cols);

    // Copy result back to host
    cudaMemcpy(output, d_output, rows * sizeof(float), cudaMemcpyDeviceToHost);

    // Free GPU memory
    cudaFree(d_mat);
    cudaFree(d_vec);
    cudaFree(d_bias);
    cudaFree(d_output);
}


#include <cmath>

#define BLOCK_SIZE 256

// Optimized CUDA Kernel for matrix-vector multiplication with bias
__global__ void cuda_mm(const float* __restrict__ mat, const float* __restrict__ vec, float* __restrict__ output, const float* __restrict__ bias, int rows, int cols) {
    extern __shared__ float shared_vec[];  // Shared memory for vector
    int row = blockIdx.x * blockDim.x + threadIdx.x;

    float sum = 0.0f;

    if (row < rows) {
        for (int col = 0; col < cols; col += blockDim.x) {
            // Load a chunk of the vector into shared memory
            int vec_idx = col + threadIdx.x;
            if (vec_idx < cols) {
                shared_vec[threadIdx.x] = vec[vec_idx];
            }
            __syncthreads();

            // Perform multiplication for the current chunk
            for (int i = 0; i < blockDim.x && (col + i) < cols; ++i) {
                sum += mat[row * cols + col + i] * shared_vec[i];
            }
            __syncthreads();
        }
        // Add bias and store the result
        output[row] = sum + bias[row];
    }
}

// Optimized CUDA Kernel for Tanh activation
__global__ void cuda_Tanh_0(float* __restrict__ data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < size) {
        float val = data[idx];
        data[idx] = tanhf(val);
    }
}

// Host function to invoke the optimized kernels
void run_optimized_kernels(const float* mat, const float* vec, const float* bias, float* output, int rows, int cols) {
    // Define grid and block dimensions
    dim3 blockDim(BLOCK_SIZE);
    dim3 gridDim_mm((rows + BLOCK_SIZE - 1) / BLOCK_SIZE);
    dim3 gridDim_tanh((rows + BLOCK_SIZE - 1) / BLOCK_SIZE);

    // Allocate memory on the GPU
    float *d_mat, *d_vec, *d_bias, *d_output;
    cudaMalloc(&d_mat, rows * cols * sizeof(float));
    cudaMalloc(&d_vec, cols * sizeof(float));
    cudaMalloc(&d_bias, rows * sizeof(float));
    cudaMalloc(&d_output, rows * sizeof(float));

    // Copy data to GPU
    cudaMemcpy(d_mat, mat, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vec, vec, cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, rows * sizeof(float), cudaMemcpyHostToDevice);

    // Launch optimized matrix-vector multiplication kernel
    size_t shared_memory_size = BLOCK_SIZE * sizeof(float);  // Shared memory size for the vector
    cuda_mm<<<gridDim_mm, blockDim, shared_memory_size>>>(d_mat, d_vec, d_output, d_bias, rows, cols);

    // Launch optimized Tanh activation kernel
    cuda_Tanh_0<<<gridDim_tanh, blockDim>>>(d_output, rows);

    // Copy result back to host
    cudaMemcpy(output, d_output, rows * sizeof(float), cudaMemcpyDeviceToHost);

    // Free GPU memory
    cudaFree(d_mat);
    cudaFree(d_vec);
    cudaFree(d_bias);
    cudaFree(d_output);
}


#include <cmath>

#define BLOCK_SIZE 256

// CUDA Kernel for matrix-vector multiplication with bias
__global__ void cuda_mm(const float* mat, const float* vec, float* output, const float* bias, int rows, int cols) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < rows) {
        float sum = 0.0f;
        for (int col = 0; col < cols; ++col) {
            sum += mat[row * cols + col] * vec[col];
        }
        // Add bias
        output[row] = sum + bias[row];
    }
}

// CUDA Kernel for Tanh activation
__global__ void cuda_Tanh_0(float* data, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < size) {
        data[idx] = tanhf(data[idx]);
    }
}

// Host code to invoke the kernels
void run_cuda_kernels(const float* mat, const float* vec, const float* bias, float* output, int rows, int cols) {
    // Define grid and block dimensions
    dim3 blockDim(BLOCK_SIZE);
    dim3 gridDim((rows + BLOCK_SIZE - 1) / BLOCK_SIZE);

    // Allocate memory on the GPU
    float *d_mat, *d_vec, *d_bias, *d_output;
    cudaMalloc(&d_mat, rows * cols * sizeof(float));
    cudaMalloc(&d_vec, cols * sizeof(float));
    cudaMalloc(&d_bias, rows * sizeof(float));
    cudaMalloc(&d_output, rows * sizeof(float));

    // Copy data to GPU
    cudaMemcpy(d_mat, mat, rows * cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_vec, vec, cols * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, rows * sizeof(float), cudaMemcpyHostToDevice);

    // Launch matrix-vector multiplication kernel
    cuda_mm<<<gridDim, blockDim>>>(d_mat, d_vec, d_output, d_bias, rows, cols);

    // Launch Tanh activation kernel
    cuda_Tanh_0<<<gridDim, blockDim>>>(d_output, rows);

    // Copy result back to host
    cudaMemcpy(output, d_output, rows * sizeof(float), cudaMemcpyDeviceToHost);

    // Free GPU memory
    cudaFree(d_mat);
    cudaFree(d_vec);
    cudaFree(d_bias);
    cudaFree(d_output);
}