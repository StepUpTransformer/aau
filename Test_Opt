#include <cuda_runtime.h>
#include <iostream>
#include <cmath>
#include <cfloat>

#define TILE_DIM 16

// Optimized CUDA Kernel for matrix-vector multiplication with bias
__global__ void mm_cuda(const float* mat, const float* vec, float* output, int rows, int cols) { 
    __shared__ float shared_vec[TILE_DIM];
    int row = blockIdx.x * blockDim.y + threadIdx.y; 
    int col_start = threadIdx.x * TILE_DIM;
    float sum = 0.0f;

    for (int i = 0; i < (cols + TILE_DIM - 1) / TILE_DIM; i++) {
        if (col_start + i < cols) {
            shared_vec[threadIdx.x] = vec[col_start + i];
        } else {
            shared_vec[threadIdx.x] = 0.0f;
        }
        __syncthreads();

        for (int j = 0; j < TILE_DIM && col_start + j < cols; ++j) {
            if (row < rows) {
                sum += mat[row * cols + col_start + j] * shared_vec[j];
            }
        }
        __syncthreads();
    }

    if (row < rows) {
        output[row] = sum;
    }
}

// Optimized CUDA Kernel for Tanh activation
__global__ void Tanh_cuda(float* data, const float* bias, int size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size) {
        data[idx] = tanhf(data[idx] + bias[idx]);
    }
}

// Optimized CUDA Kernel for matrix-vector multiplication with bias
__global__ void addmm_cuda(const float* mat, const float* vec, const float* bias, float* output, int rows, int cols) { 
    __shared__ float shared_vec[TILE_DIM];
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    int col_start = threadIdx.x * TILE_DIM;
    float sum = 0.0f;

    if (row < rows) {
        sum = bias[row];
    }

    for (int i = 0; i < (cols + TILE_DIM - 1) / TILE_DIM; i++) {
        if (col_start + i < cols) {
            shared_vec[threadIdx.x] = vec[col_start + i];
        } else {
            shared_vec[threadIdx.x] = 0.0f;
        }
        __syncthreads();

        for (int j = 0; j < TILE_DIM && col_start + j < cols; ++j) {
            if (row < rows) {
                sum += mat[row * cols + col_start + j] * shared_vec[j];
            }
        }
        __syncthreads();
    }

    if (row < rows) {
        output[row] = sum;
    }
}

// Optimized CUDA Kernel for Softmax
__global__ void softmax_1_cuda(float* output, float* softmax_out, int output_size) {
    __shared__ float shared_output[TILE_DIM];
    int idx = threadIdx.x;
    int global_idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < output_size) {
        shared_output[idx] = output[global_idx];
    }
    __syncthreads();

    float max_val = -FLT_MAX;
    if (idx < output_size) {
        for (int i = 0; i < output_size; ++i) {
            max_val = max(max_val, shared_output[i]);
        }
    }
    __syncthreads();

    float sum_exp = 0.0f;
    if (idx < output_size) {
        for (int i = 0; i < output_size; ++i) {
            sum_exp += expf(shared_output[i] - max_val);
        }
    }
    __syncthreads();

    if (idx < output_size) {
        softmax_out[global_idx] = expf(shared_output[idx] - max_val) / sum_exp;
    }
}

// Main function remains mostly unchanged, except kernel launch configurations
int main() {
    const int input_size = 100; 
    const int hidden_size = 200; 
    const int output_size = 10; 
    float *d_softmax_out;

    // Allocate host memory
    float *h_input = new float[input_size]; 
    float *h_hidden = new float[hidden_size];
    float *h_output = new float[output_size];
    float *h_linear1_weights = new float[input_size * hidden_size];
    float *h_linear1_bias = new float[hidden_size];
    float *h_linear2_weights = new float[hidden_size * output_size];
    float *h_linear2_bias = new float[output_size];

    // Initialize data
    initializeData(h_input, h_linear1_weights, h_linear1_bias, h_linear2_weights, h_linear2_bias, input_size, hidden_size, output_size);

    // Allocate device memory
    float *d_input, *d_hidden, *d_output;
    float *d_linear1_weights, *d_linear1_bias;
    float *d_linear2_weights, *d_linear2_bias;

    cudaMalloc(&d_input, input_size * sizeof(float)); 
    cudaMalloc(&d_hidden, hidden_size * sizeof(float));
    cudaMalloc(&d_output, output_size * sizeof(float));

    cudaMalloc(&d_linear1_weights, input_size * hidden_size * sizeof(float));
    cudaMalloc(&d_linear1_bias, hidden_size * sizeof(float));
    cudaMalloc(&d_linear2_weights, hidden_size * output_size * sizeof(float));
    cudaMalloc(&d_linear2_bias, output_size * sizeof(float));
    cudaMalloc((void**)&d_softmax_out, output_size * sizeof(float));

    // Copy data to device
    cudaMemcpy(d_input, h_input, input_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear1_weights, h_linear1_weights, input_size * hidden_size * sizeof(float), cudaMemcpyHostToDevice); 
    cudaMemcpy(d_linear1_bias, h_linear1_bias, hidden_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear2_weights, h_linear2_weights, hidden_size * output_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_linear2_bias, h_linear2_bias, output_size * sizeof(float), cudaMemcpyHostToDevice);

    // Launch optimized kernels
    dim3 mm_gridDim((hidden_size + TILE_DIM - 1) / TILE_DIM, 1, 1);
    dim3 mm_blockDim(TILE_DIM, TILE_DIM, 1);
    mm_cuda<<<mm_gridDim, mm_blockDim>>>(d_linear1_weights, d_input, d_hidden, hidden_size, input_size);

    dim3 Tanh_gridDim((hidden_size + TILE_DIM - 1) / TILE_DIM, 1, 1);
    dim3 Tanh_blockDim(TILE_DIM, 1, 1);
    Tanh_cuda<<<Tanh_gridDim, Tanh_blockDim>>>(d_hidden, d_linear1_bias, hidden_size);

    dim3 addmm_gridDim((output_size + TILE_DIM - 1) / TILE_DIM, 1, 1);
    dim3 addmm_blockDim(TILE_DIM, TILE_DIM, 1);
    addmm_cuda<<<addmm_gridDim, addmm_blockDim>>>(d_linear2_weights, d_hidden, d_linear2_bias, d_output, output_size, hidden_size);

    dim3 softmax_gridDim((output_size + TILE_DIM - 1) / TILE_DIM, 1, 1);
    dim3 softmax_blockDim(TILE_DIM, 1, 1);
    softmax_1_cuda<<<softmax_gridDim, softmax_blockDim>>>(d_output, d_softmax_out, output_size);

    // Copy results back to host
    cudaMemcpy(h_output, d_softmax_out, output_size * sizeof(float), cudaMemcpyDeviceToHost);

    // Output the results
    std::cout << "\nOutput: " << std::endl;
    for (int i = 0; i < output_size; ++i) {
        std::cout << h_output[i] << " ";
    }
    std::cout << std::endl;

    // Clean up
    delete[] h_input;
    delete[] h_hidden;
    delete[] h_output;
    delete[] h_linear1_weights;
    delete[] h_linear1_bias;
    delete[] h_linear2_weights;
    delete[] h_linear2_bias;

    cudaFree(d_input);
    cudaFree(d_hidden);
    cudaFree(d_output);
    cudaFree(d_linear1_weights);
    cudaFree(d_linear1_bias);
    cudaFree(d_linear2_weights);
    cudaFree(d_linear2_bias);

    return 0;
}
