__global__ void softmax_optimized(float *output, float *softmax_out, int output_size) {
    extern __shared__ float shared_data[]; // Shared memory for temporary storage

    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    int tid = threadIdx.x;

    if (idx >= output_size) return;

    // Step 1: Find the maximum value in the output array (per block)
    float max_val = -FLT_MAX;
    for (int i = tid; i < output_size; i += blockDim.x) {
        max_val = fmaxf(max_val, output[i]);
    }

    // Reduce max_val across the block
    shared_data[tid] = max_val;
    __syncthreads();

    for (int offset = blockDim.x / 2; offset > 0; offset /= 2) {
        if (tid < offset) {
            shared_data[tid] = fmaxf(shared_data[tid], shared_data[tid + offset]);
        }
        __syncthreads();
    }
    max_val = shared_data[0]; // The maximum value in the block

    // Step 2: Compute the sum of exponentials (per block)
    float sum_exp = 0.0f;
    for (int i = tid; i < output_size; i += blockDim.x) {
        sum_exp += expf(output[i] - max_val);
    }

    // Reduce sum_exp across the block
    shared_data[tid] = sum_exp;
    __syncthreads();

    for (int offset = blockDim.x / 2; offset > 0; offset /= 2) {
        if (tid < offset) {
            shared_data[tid] += shared_data[tid + offset];
        }
        __syncthreads();
    }
    sum_exp = shared_data[0]; // The sum of exponentials in the block

    // Step 3: Compute the softmax output
    if (idx < output_size) {
        softmax_out[idx] = expf(output[idx] - max_val) / sum_exp;
    }
}

#include <cuda_runtime.h>
#include <float.h>

__global__ void softmax_ultra(float *output, float *softmax_out, int output_size) {
    extern __shared__ float shared_data[]; // Shared memory for reduction

    int tid = threadIdx.x;
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    int block_size = blockDim.x;

    // Step 1: Load the output values into shared memory and compute the maximum in parallel
    float local_max = -FLT_MAX;
    for (int i = tid; i < output_size; i += block_size) {
        local_max = fmaxf(local_max, output[i]);
    }

    // Reduce max across the block using warp-level primitives
    shared_data[tid] = local_max;
    __syncthreads();
    for (int stride = block_size / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            shared_data[tid] = fmaxf(shared_data[tid], shared_data[tid + stride]);
        }
        __syncthreads();
    }
    float max_val = shared_data[0];

    // Step 2: Compute the sum of exponentials in parallel
    float local_sum = 0.0f;
    for (int i = tid; i < output_size; i += block_size) {
        local_sum += expf(output[i] - max_val);
    }

    // Reduce sum across the block
    shared_data[tid] = local_sum;
    __syncthreads();
    for (int stride = block_size / 2; stride > 0; stride /= 2) {
        if (tid < stride) {
            shared_data[tid] += shared_data[tid + stride];
        }
        __syncthreads();
    }
    float sum_exp = shared_data[0];

    // Step 3: Write the normalized softmax output
    for (int i = tid; i < output_size; i += block_size) {
        softmax_out[i] = expf(output[i] - max_val) / sum_exp;
    }
}